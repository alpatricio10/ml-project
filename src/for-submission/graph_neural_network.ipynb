{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import optuna\n",
    "from tqdm.notebook import tqdm\n",
    "import ast\n",
    "from functools import partial\n",
    "import warnings\n",
    "import networkx as nx\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Make sure to change the name of the train and test dataset files to the directory that has the file. By default, it is located in the file that has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load the dataset and convert edge lists to the correct format.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['edgelist'] = df['edgelist'].apply(ast.literal_eval)\n",
    "    return df\n",
    "\n",
    "# Load training data\n",
    "train_df = load_data('./data/train.csv')\n",
    "print(f\"Loaded {len(train_df)} training samples\")\n",
    "\n",
    "# Load test data\n",
    "test_df = load_data('./data/test.csv')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Network\n",
    "The following blocks show the setup of the structure and features of the GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial feature transformation\n",
    "        self.lin1 = torch.nn.Linear(num_node_features, hidden_channels)\n",
    "        \n",
    "        # GCN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Output layers\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = torch.nn.Linear(hidden_channels, 1)\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Initial feature transformation\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        # Reshape output to match target dimensions\n",
    "        x = x.squeeze(-1)\n",
    "        \n",
    "        # Output log probabilities\n",
    "        return F.log_softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_features(num_nodes, edge_index, device, language=None):\n",
    "    \"\"\"Create node features with different options for feature engineering.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Create NetworkX graph for centrality calculations\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    edges = edge_index.cpu().numpy().T\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Closeness centrality\n",
    "    closeness = nx.closeness_centrality(G)\n",
    "    closeness_tensor = torch.tensor([closeness[i] for i in range(num_nodes)], device=device)\n",
    "    features.append(closeness_tensor.unsqueeze(1))\n",
    "\n",
    "    # Betweenness centrality\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    betweenness_tensor = torch.tensor([betweenness[i] for i in range(num_nodes)], device=device)\n",
    "    features.append(betweenness_tensor.unsqueeze(1))\n",
    "        \n",
    "    # PageRank centrality\n",
    "    pagerank = nx.pagerank(G)\n",
    "    pagerank_tensor = torch.tensor([pagerank[i] for i in range(num_nodes)], device=device)\n",
    "    features.append(pagerank_tensor.unsqueeze(1))\n",
    "\n",
    "    # Eigenvector centrality\n",
    "    eigenvector = nx.eigenvector_centrality(G,  max_iter=1000, tol=1e-04)\n",
    "    eigenvector_tensor = torch.tensor([eigenvector[i] for i in range(num_nodes)], device=device)\n",
    "    features.append(eigenvector_tensor.unsqueeze(1))\n",
    "        \n",
    "    # Eccentricity\n",
    "    eccentricity = nx.eccentricity(G)\n",
    "    eccentricity_tensor = torch.tensor([eccentricity[i] for i in range(num_nodes)], device=device)\n",
    "    features.append(eccentricity_tensor.unsqueeze(1))\n",
    "    \n",
    "    # Add language-based features if languages are provided\n",
    "    if language is not None:\n",
    "        # Language groups mapping\n",
    "        language_groups = {\n",
    "            'head_final_sov': ['Japanese', 'Korean', 'Turkish'],\n",
    "            'romance_svo': ['Spanish', 'Portuguese', 'French', 'Italian', 'Galician'],\n",
    "            'germanic_v2': ['German', 'Swedish', 'Icelandic'],\n",
    "            'free_order_case': ['Russian', 'Polish', 'Czech', 'Finnish'],\n",
    "            'analytic': ['English', 'Chinese', 'Thai', 'Indonesian'],\n",
    "            'other': ['Arabic', 'Hindi']\n",
    "        }\n",
    "        \n",
    "        # Create one-hot encoding for languages\n",
    "        one_hot = torch.zeros((num_nodes, 21), device=device)\n",
    "        lang_idx = list(language_groups.values())[0].index(language) if language in list(language_groups.values())[0] else 20\n",
    "        one_hot[:, lang_idx] = 1\n",
    "        features.append(one_hot)\n",
    "        \n",
    "        # Create language group features\n",
    "        group_features = torch.zeros((num_nodes, len(language_groups)), device=device) \n",
    "        for group_idx, (group_name, group_langs) in enumerate(language_groups.items()):\n",
    "            if language in group_langs:\n",
    "                group_features[:, group_idx] = 1\n",
    "                break\n",
    "        features.append(group_features)\n",
    "    \n",
    "    return torch.cat(features, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pytorch_geometric_data(df_row, device, feature_type='basic'):\n",
    "    \"\"\"Convert a single graph data row to PyTorch Geometric Data object.\"\"\"\n",
    "    edge_list = df_row['edgelist']\n",
    "    num_nodes = df_row['n']\n",
    "    languages = df_row['language']\n",
    "    \n",
    "    # Only get root if present (i.e., not in test set)\n",
    "    root = df_row['root'] - 1 if 'root' in df_row else None\n",
    "\n",
    "    # Create edge index with both directions for undirected graph\n",
    "    edges = []\n",
    "    for src, dst in edge_list:\n",
    "        edges.append([src-1, dst-1])  # forward edge\n",
    "        edges.append([dst-1, src-1])  # backward edge\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long, device=device).t()\n",
    "\n",
    "    # Create node features\n",
    "    x = create_node_features(num_nodes, edge_index, device, languages)\n",
    "\n",
    "    # Create Data object\n",
    "    if root is not None:\n",
    "        y = torch.tensor([root], dtype=torch.long, device=device)\n",
    "        return Data(x=x, edge_index=edge_index, y=y, language=languages)\n",
    "    else:\n",
    "        return Data(x=x, edge_index=edge_index, language=languages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_with_metrics(model, val_data, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_languages = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_data:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index)\n",
    "            \n",
    "            target = data.y.view(-1)\n",
    "            out = out.unsqueeze(0)\n",
    "            \n",
    "            loss = F.nll_loss(out, target)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred = out.squeeze(0).argmax(dim=0)\n",
    "            \n",
    "            # Convert to binary classification (root vs non-root)\n",
    "            # Create binary labels for all nodes\n",
    "            num_nodes = len(data.x)\n",
    "            binary_target = torch.zeros(num_nodes, device=device)\n",
    "            binary_target[target] = 1\n",
    "            \n",
    "            binary_pred = torch.zeros(num_nodes, device=device)\n",
    "            binary_pred[pred] = 1\n",
    "            \n",
    "            all_targets.extend(binary_target.cpu().numpy())\n",
    "            all_preds.extend(binary_pred.cpu().numpy())\n",
    "            \n",
    "            # Get language from the data if available\n",
    "            if hasattr(data, 'language'):\n",
    "                all_languages.extend([data.language] * num_nodes)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    accuracy = sum(p == t for p, t in zip(all_preds, all_targets)) / len(all_targets)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    \n",
    "    # Calculate per-language metrics if languages are available\n",
    "    per_language_metrics = {}\n",
    "    if all_languages:\n",
    "        unique_languages = set(all_languages)\n",
    "        for lang in unique_languages:\n",
    "            lang_mask = [l == lang for l in all_languages]\n",
    "            lang_targets = [t for t, m in zip(all_targets, lang_mask) if m]\n",
    "            lang_preds = [p for p, m in zip(all_preds, lang_mask) if m]\n",
    "            \n",
    "            if len(lang_targets) > 0:  # Only calculate metrics if we have samples for this language\n",
    "                per_language_metrics[lang] = {\n",
    "                    'precision': precision_score(lang_targets, lang_preds, zero_division=0),\n",
    "                    'recall': recall_score(lang_targets, lang_preds, zero_division=0),\n",
    "                    'f1': f1_score(lang_targets, lang_preds, zero_division=0)\n",
    "                }\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(val_data),\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'per_language_metrics': per_language_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_data, device, class_weights=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data in train_data:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        \n",
    "        target = data.y.view(-1)\n",
    "        out = out.unsqueeze(0)\n",
    "\n",
    "        if class_weights is not None:\n",
    "            weight = class_weights[target.item()]\n",
    "            loss = F.nll_loss(out, target) * weight\n",
    "        else:\n",
    "            loss = F.nll_loss(out, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = out.squeeze(0).argmax(dim=0)\n",
    "        correct += int(pred == target)\n",
    "    \n",
    "    return total_loss / len(train_data), correct / len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Hyperparamters\n",
    "\n",
    "- F1-score: 0.2783809523809524\n",
    "- Params: \n",
    "  - hidden_channels: 128\n",
    "  - num_layers: 2\n",
    "  - dropout_rate: 0.24007533818963195\n",
    "  - learning_rate: 0.00967001569505872\n",
    "  - weight_decay: 0.00017412411662641109"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset\n",
    "dataset = [create_pytorch_geometric_data(row, device) \n",
    "          for _, row in train_df.iterrows()]\n",
    "\n",
    "# Initialize model with best parameters\n",
    "num_node_features = dataset[0].x.size(1)\n",
    "\n",
    "# Train final model with best parameters\n",
    "final_model = GraphConvolutionNetwork(\n",
    "    num_node_features=num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    "    dropout_rate=0.24007533818963195\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    final_model.parameters(),\n",
    "    lr= 0.00967001569505872,\n",
    "    weight_decay=0.00017412411662641109\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation split (3 folds for training, 1 for validation)\n",
    "n_folds = 4\n",
    "kf = GroupKFold(n_splits=n_folds)\n",
    "groups = train_df['sentence'].values\n",
    "\n",
    "# Use the last fold as validation set\n",
    "val_fold = n_folds - 1  # This will be fold 3 (0-based indexing)\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset, groups=groups)):\n",
    "    if fold == val_fold:\n",
    "        val_data = [dataset[i] for i in val_idx]\n",
    "    else:\n",
    "        train_data.extend([dataset[i] for i in train_idx])\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "\n",
    "# Training loop\n",
    "best_f1_score = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(final_model, optimizer, train_data, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = validate_with_metrics(final_model, val_data, device)\n",
    "    val_f1 = val_metrics['f1_score']\n",
    "    \n",
    "    print(f'Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val F1: {val_f1:.4f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_f1 > best_f1_score:\n",
    "        best_f1_score = val_f1\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(final_model.state_dict(), 'validation_gcn.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 10:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate final performance\n",
    "final_model.load_state_dict(torch.load('validation_gcn.pt'))\n",
    "final_metrics = validate_with_metrics(final_model, val_data, device)\n",
    "\n",
    "print(\"\\nFinal Validation Performance:\")\n",
    "print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {final_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {final_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {final_metrics['f1_score']:.4f}\")\n",
    "    \n",
    "# Print per-language metrics\n",
    "print(\"\\nPer-Language Performance:\")\n",
    "for lang, metrics in final_metrics['per_language_metrics'].items():\n",
    "    print(f\"\\n{lang}:\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_f1_score = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(\n",
    "                    final_model, optimizer, train_data, device)\n",
    "    \n",
    "    # Validation with F1 score\n",
    "    val_metrics = validate_with_metrics(final_model, dataset, device)\n",
    "    val_f1 = val_metrics['f1_score']\n",
    "    \n",
    "    print(f'Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val F1: {val_f1:.4f}')\n",
    "    \n",
    "    # Early stopping based on F1 score\n",
    "    if val_f1 > best_f1_score:\n",
    "        best_f1_score = val_f1\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(final_model.state_dict(), 'best_model_gcn.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 10:\n",
    "            print('Early stopping!')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "final_model.load_state_dict(torch.load('best_model_gcn.pt'))\n",
    "final_model.eval()\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for _, row in test_df.iterrows():\n",
    "        data = create_pytorch_geometric_data(row, device)\n",
    "        data = data.to(device)\n",
    "        out = final_model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=0).item() + 1  # Convert back to 1-based indexing\n",
    "        predictions.append(pred)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': range(1, len(predictions) + 1),\n",
    "    'root': predictions\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Predictions saved to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
