{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import optuna\n",
    "from tqdm.notebook import tqdm\n",
    "import ast\n",
    "from functools import partial\n",
    "import warnings\n",
    "import networkx as nx\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Make sure to change the name of the train and test dataset files to the directory that has the file. By default, it is located in the file that has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load the dataset and convert edge lists to the correct format.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['edgelist'] = df['edgelist'].apply(ast.literal_eval)\n",
    "    return df\n",
    "\n",
    "# Load training data\n",
    "train_df = load_data('./data/train.csv')\n",
    "print(f\"Loaded {len(train_df)} training samples\")\n",
    "\n",
    "# Load test data\n",
    "test_df = load_data('./data/test.csv')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Network\n",
    "The following blocks show the setup of the structure and features of the GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial feature transformation\n",
    "        self.lin1 = torch.nn.Linear(num_node_features, hidden_channels)\n",
    "        \n",
    "        # GCN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Output layers\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = torch.nn.Linear(hidden_channels, 1)\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Initial feature transformation\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        # Reshape output to match target dimensions\n",
    "        x = x.squeeze(-1)\n",
    "        \n",
    "        # Output log probabilities\n",
    "        return F.log_softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_features(num_nodes, edge_index, device, language=None):\n",
    "    \"\"\"Create node features with different options for feature engineering.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Create NetworkX graph for centrality calculations\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    edges = edge_index.cpu().numpy().T\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Closeness centrality\n",
    "    closeness = nx.closeness_centrality(G)\n",
    "    closeness_tensor = torch.tensor([closeness[i] for i in range(num_nodes)], device=device)\n",
    "    features.append(closeness_tensor.unsqueeze(1))\n",
    "\n",
    "    # Betweenness centrality\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    betweenness_tensor = torch.tensor([betweenness[i] for i in range(num_nodes)], device=device)\n",
    "    features.append(betweenness_tensor.unsqueeze(1))\n",
    "        \n",
    "    # PageRank centrality\n",
    "    pagerank = nx.pagerank(G)\n",
    "    pagerank_tensor = torch.tensor([pagerank[i] for i in range(num_nodes)], device=device)\n",
    "    features.append(pagerank_tensor.unsqueeze(1))\n",
    "\n",
    "    # Eigenvector centrality\n",
    "    eigenvector = nx.eigenvector_centrality(G,  max_iter=1000, tol=1e-04)\n",
    "    eigenvector_tensor = torch.tensor([eigenvector[i] for i in range(num_nodes)], device=device)\n",
    "    features.append(eigenvector_tensor.unsqueeze(1))\n",
    "        \n",
    "    # Eccentricity\n",
    "    eccentricity = nx.eccentricity(G)\n",
    "    eccentricity_tensor = torch.tensor([eccentricity[i] for i in range(num_nodes)], device=device)\n",
    "    features.append(eccentricity_tensor.unsqueeze(1))\n",
    "    \n",
    "    # Add language-based features if languages are provided\n",
    "    if language is not None:\n",
    "        # Language groups mapping\n",
    "        language_groups = {\n",
    "            'head_final_sov': ['Japanese', 'Korean', 'Turkish'],\n",
    "            'romance_svo': ['Spanish', 'Portuguese', 'French', 'Italian', 'Galician'],\n",
    "            'germanic_v2': ['German', 'Swedish', 'Icelandic'],\n",
    "            'free_order_case': ['Russian', 'Polish', 'Czech', 'Finnish'],\n",
    "            'analytic': ['English', 'Chinese', 'Thai', 'Indonesian'],\n",
    "            'other': ['Arabic', 'Hindi']\n",
    "        }\n",
    "        \n",
    "        # Create one-hot encoding for languages\n",
    "        one_hot = torch.zeros((num_nodes, 21), device=device)\n",
    "        lang_idx = list(language_groups.values())[0].index(language) if language in list(language_groups.values())[0] else 20\n",
    "        one_hot[:, lang_idx] = 1\n",
    "        features.append(one_hot)\n",
    "        \n",
    "        # Create language group features\n",
    "        group_features = torch.zeros((num_nodes, len(language_groups)), device=device) \n",
    "        for group_idx, (group_name, group_langs) in enumerate(language_groups.items()):\n",
    "            if language in group_langs:\n",
    "                group_features[:, group_idx] = 1\n",
    "                break\n",
    "        features.append(group_features)\n",
    "    \n",
    "    return torch.cat(features, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pytorch_geometric_data(df_row, device, feature_type='basic'):\n",
    "    \"\"\"Convert a single graph data row to PyTorch Geometric Data object.\"\"\"\n",
    "    edge_list = df_row['edgelist']\n",
    "    num_nodes = df_row['n']\n",
    "    languages = df_row['language']\n",
    "    \n",
    "    # Only get root if present (i.e., not in test set)\n",
    "    root = df_row['root'] - 1 if 'root' in df_row else None\n",
    "\n",
    "    # Create edge index with both directions for undirected graph\n",
    "    edges = []\n",
    "    for src, dst in edge_list:\n",
    "        edges.append([src-1, dst-1])  # forward edge\n",
    "        edges.append([dst-1, src-1])  # backward edge\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long, device=device).t()\n",
    "\n",
    "    # Create node features\n",
    "    x = create_node_features(num_nodes, edge_index, device, languages)\n",
    "\n",
    "    # Create Data object\n",
    "    if root is not None:\n",
    "        y = torch.tensor([root], dtype=torch.long, device=device)\n",
    "        return Data(x=x, edge_index=edge_index, y=y, language=languages)\n",
    "    else:\n",
    "        return Data(x=x, edge_index=edge_index, language=languages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_with_metrics(model, val_data, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_languages = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_data:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index)\n",
    "            \n",
    "            target = data.y.view(-1)\n",
    "            out = out.unsqueeze(0)\n",
    "            \n",
    "            loss = F.nll_loss(out, target)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred = out.squeeze(0).argmax(dim=0)\n",
    "            \n",
    "            # Convert to binary classification (root vs non-root)\n",
    "            # Create binary labels for all nodes\n",
    "            num_nodes = len(data.x)\n",
    "            binary_target = torch.zeros(num_nodes, device=device)\n",
    "            binary_target[target] = 1\n",
    "            \n",
    "            binary_pred = torch.zeros(num_nodes, device=device)\n",
    "            binary_pred[pred] = 1\n",
    "            \n",
    "            all_targets.extend(binary_target.cpu().numpy())\n",
    "            all_preds.extend(binary_pred.cpu().numpy())\n",
    "            \n",
    "            # Get language from the data if available\n",
    "            if hasattr(data, 'language'):\n",
    "                all_languages.extend([data.language] * num_nodes)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    accuracy = sum(p == t for p, t in zip(all_preds, all_targets)) / len(all_targets)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    \n",
    "    # Calculate per-language metrics if languages are available\n",
    "    per_language_metrics = {}\n",
    "    if all_languages:\n",
    "        unique_languages = set(all_languages)\n",
    "        for lang in unique_languages:\n",
    "            lang_mask = [l == lang for l in all_languages]\n",
    "            lang_targets = [t for t, m in zip(all_targets, lang_mask) if m]\n",
    "            lang_preds = [p for p, m in zip(all_preds, lang_mask) if m]\n",
    "            \n",
    "            if len(lang_targets) > 0:  # Only calculate metrics if we have samples for this language\n",
    "                per_language_metrics[lang] = {\n",
    "                    'precision': precision_score(lang_targets, lang_preds, zero_division=0),\n",
    "                    'recall': recall_score(lang_targets, lang_preds, zero_division=0),\n",
    "                    'f1': f1_score(lang_targets, lang_preds, zero_division=0)\n",
    "                }\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(val_data),\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'per_language_metrics': per_language_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_data, device, class_weights=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data in train_data:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        \n",
    "        target = data.y.view(-1)\n",
    "        out = out.unsqueeze(0)\n",
    "\n",
    "        if class_weights is not None:\n",
    "            weight = class_weights[target.item()]\n",
    "            loss = F.nll_loss(out, target) * weight\n",
    "        else:\n",
    "            loss = F.nll_loss(out, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = out.squeeze(0).argmax(dim=0)\n",
    "        correct += int(pred == target)\n",
    "    \n",
    "    return total_loss / len(train_data), correct / len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_df, device, feature_type='basic'):\n",
    "    params = {\n",
    "        'hidden_channels': trial.suggest_categorical('hidden_channels', [64, 128, 256]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 2, 4),  \n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.2, 0.4),  \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 3e-4, 1e-2, log=True),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-4, 1e-3, log=True),\n",
    "        'max_epochs': 100,  \n",
    "        'patience': 10,  \n",
    "        'batch_size': 32\n",
    "    }\n",
    "    \n",
    "    # Create folds\n",
    "    n_folds = 4\n",
    "    kf = GroupKFold(n_splits=n_folds)\n",
    "    \n",
    "    # Convert data to PyG format\n",
    "    dataset = [create_pytorch_geometric_data(row, device, feature_type) \n",
    "              for _, row in train_df.iterrows()]\n",
    "    \n",
    "    # Group by sentence for GroupKFold\n",
    "    groups = train_df['sentence'].values\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset, groups=groups)):\n",
    "        trial.set_user_attr(f'fold_{fold+1}_status', 'starting')\n",
    "        \n",
    "        try:\n",
    "            # Split data\n",
    "            train_data = [dataset[i] for i in train_idx]\n",
    "            val_data = [dataset[i] for i in val_idx]\n",
    "            \n",
    "            # Initialize model\n",
    "            num_node_features = dataset[0].x.size(1)\n",
    "            model = GraphConvolutionNetwork(\n",
    "                num_node_features=num_node_features,\n",
    "                hidden_channels=params['hidden_channels'],\n",
    "                num_layers=params['num_layers'],\n",
    "                dropout_rate=params['dropout_rate']\n",
    "            ).to(device)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=params['learning_rate'],\n",
    "                weight_decay=params['weight_decay']\n",
    "            )\n",
    "            \n",
    "            # Training loop with early stopping\n",
    "            best_f1_score = 0  \n",
    "            patience_counter = 0\n",
    "        \n",
    "            for epoch in range(params['max_epochs']):\n",
    "                train_loss, train_acc = train_epoch(\n",
    "                    model, optimizer, train_data, device)\n",
    "                val_metrics = validate_with_metrics(model, val_data, device)\n",
    "    \n",
    "                # Use F1 score for early stopping\n",
    "                if val_metrics['f1_score'] > best_f1_score:\n",
    "                    best_f1_score = val_metrics['f1_score']\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= params['patience']:\n",
    "                        break\n",
    "                \n",
    "                # Report F1 score to Optuna\n",
    "                trial.report(val_metrics['f1_score'], epoch)\n",
    "                \n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "            \n",
    "            scores.append(best_f1_score)\n",
    "            trial.set_user_attr(f'fold_{fold+1}_score', best_f1_score)\n",
    "            trial.set_user_attr(f'fold_{fold+1}_status', 'completed')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} Fold {fold+1} failed with exception: {str(e)}\")\n",
    "            trial.set_user_attr(f'fold_{fold+1}_status', f'failed_exception: {str(e)}')\n",
    "            continue\n",
    "    \n",
    "    if not scores:\n",
    "        raise optuna.TrialPruned()\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create study\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5,\n",
    "        n_warmup_steps=2,\n",
    "        interval_steps=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create objective function with fixed parameters\n",
    "objective_func = partial(objective, train_df=train_df, device=device)\n",
    "\n",
    "# Optimize\n",
    "study.optimize(objective_func, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  F1-score: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
