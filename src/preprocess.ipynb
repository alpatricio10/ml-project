{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b0e1c5",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20adde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d330d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_edgelist(edgelist_str):\n",
    "    \"\"\"\n",
    "    - Parse the edge list string to a list of tuples\n",
    "    \"\"\"\n",
    "    return ast.literal_eval(edgelist_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a409523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralities(edgelist):\n",
    "    \"\"\"\n",
    "    - edgelist is a list of node pairs e.g. [(7,2),(1,7),(1,9),...]\n",
    "    - returns a dictionary of vertex -> (centrality values)\n",
    "    \"\"\"\n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    # dc = nx.degree_centrality(T)\n",
    "    cc = nx.closeness_centrality(T)\n",
    "    bc = nx.betweenness_centrality(T)\n",
    "    pc = nx.pagerank(T)\n",
    "\n",
    "    # additional features\n",
    "    # ec = nx.eccentricity(T)\n",
    "    lc = nx.load_centrality(T)\n",
    "    eic = nx.eigenvector_centrality(T, max_iter=3000)\n",
    "    ap = list(nx.articulation_points(T))\n",
    "\n",
    "    return {v: (cc[v], bc[v], pc[v], lc[v], eic[v], 1 if v in ap else 0) for v in T}\n",
    "\n",
    "    # return {v: (dc[v], cc[v], bc[v], pc[v], ec[v], lc[v], 1 if v in ap else 0) for v in T}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(data):\n",
    "    \"\"\"Generate new features for the dataset\"\"\"\n",
    "    prepared_data = data.copy()\n",
    "\n",
    "    # Language - one-hot encoding\n",
    "    # The first one is dropped since it does not add more info (all 0 implies the other language)\n",
    "    encoded_language = pd.get_dummies(prepared_data['language'], prefix='lang', drop_first=True, dtype=int)\n",
    "    prepared_data = pd.concat([prepared_data, encoded_language], axis=1)\n",
    "\n",
    "    # Language - label encoding\n",
    "    # prepared_data['language_code'] = pd.factorize(prepared_data['language'])[0]  \n",
    "\n",
    "    return prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cafdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_classification_dataset(data, is_test=False):\n",
    "    \"\"\"\n",
    "    - extracts a feature set from a set of edge lists\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        language = row['language']\n",
    "        sentence_id = row['sentence']\n",
    "        n = row['n']  \n",
    "        edgelist = parse_edgelist(row['edgelist'])\n",
    "        root = int(row['root']) if not is_test else None\n",
    "\n",
    "        cent_dict = centralities(edgelist)\n",
    "        \n",
    "        for vertex, (closeness, betweenness, pagerank, load, eigenvector, is_articulation) in cent_dict.items():\n",
    "            row_data = {\n",
    "                'language': language,\n",
    "                'sentence': sentence_id,\n",
    "                'n': n,\n",
    "                'vertex': vertex,\n",
    "                'closeness': closeness,\n",
    "                'betweenness': betweenness,\n",
    "                'pagerank': pagerank,\n",
    "                'load': load,\n",
    "                'eigenvector': eigenvector,\n",
    "                'is_articulation': is_articulation,\n",
    "            }\n",
    "\n",
    "            if is_test:\n",
    "                row_data['id'] = row['id']\n",
    "            else:\n",
    "                row_data['is_root'] = 1 if vertex == root else 0\n",
    "            \n",
    "            all_rows.append(row_data)\n",
    "    \n",
    "    return generate_features(pd.DataFrame(all_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fe0c6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9169204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71740c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary classification dataset from training data\n",
    "train_processed = generate_binary_classification_dataset(train_data)\n",
    "\n",
    "# Load the transformed data to file\n",
    "train_processed.to_csv('../data/train_processed.csv', index=False)\n",
    "\n",
    "train_processed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1db68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# Generate binary classification dataset from training data\n",
    "test_processed = generate_binary_classification_dataset(test_data, True)\n",
    "\n",
    "# Load the transformed data to file\n",
    "test_processed.to_csv('../data/test_processed.csv', index=False)\n",
    "\n",
    "test_processed.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
