{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import set_config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from functools import partial\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_root_f1_scorer(estimator, X_with_groups_and_features, y_true, feature_columns_for_model):\n",
    "    \"\"\"Custom scorer. X_with_groups_and_features should be a pandas DataFrame containing\n",
    "       both group columns ('language', 'sentence') and the feature_columns_for_model.\"\"\"\n",
    "\n",
    "    if not isinstance(X_with_groups_and_features, pd.DataFrame):\n",
    "        raise ValueError(\"X_with_groups_and_features must be a pandas DataFrame for this scorer version.\")\n",
    "\n",
    "    # Make predictions using only the specified feature columns\n",
    "    X_for_predict = X_with_groups_and_features[feature_columns_for_model]\n",
    "    try:\n",
    "        proba_values = estimator.predict_proba(X_for_predict)[:, 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during predict_proba in scorer: {e}. Columns expected: {feature_columns_for_model}, Columns in X_for_predict: {X_for_predict.columns.tolist() if isinstance(X_for_predict, pd.DataFrame) else 'N/A (NumPy)'}\")\n",
    "        return 0.0 # Return a score that indicates failure\n",
    "\n",
    "    # Add probabilities to a copy of the DataFrame to use its index for grouping and selection\n",
    "    df_copy = X_with_groups_and_features.copy()\n",
    "    df_copy['__proba__'] = proba_values\n",
    "\n",
    "    y_pred = pd.Series(np.zeros(len(df_copy)), index=df_copy.index)\n",
    "    group_cols = ['language', 'sentence']\n",
    "\n",
    "    for _, group_df in df_copy.groupby(group_cols):\n",
    "        if not group_df.empty:\n",
    "            if '__proba__' in group_df.columns:\n",
    "                idx_max = group_df['__proba__'].idxmax()\n",
    "                y_pred.loc[idx_max] = 1\n",
    "            else:\n",
    "                # This case should not happen if proba_values were assigned correctly\n",
    "                print(f\"Warning: '__proba__' column missing in group_df during scoring.\")\n",
    "    \n",
    "    # Ensure y_true and y_pred are numpy arrays for f1_score to avoid potential index issues\n",
    "    return f1_score(y_true.values, y_pred.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(df, n_folds, group_column):\n",
    "    \"\"\"Create folds for GroupKFold cross-validation\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy['kfold'] = -1\n",
    "    y = df_copy['is_root']\n",
    "    groups = df_copy[group_column]\n",
    "\n",
    "    kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X=df_copy, y=y, groups=groups)):\n",
    "        df_copy.loc[val_idx, 'kfold'] = fold\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def run_model(df_with_folds, fold, estimator, params, feature_columns, feature_pipeline=None, trial=None):\n",
    "    \"\"\"Run model training and evaluation for a single fold.\"\"\"\n",
    "    df_train = df_with_folds[df_with_folds.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df_with_folds[df_with_folds.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    X_train_original = df_train.drop(columns=['is_root', 'kfold'])\n",
    "    X_valid_original = df_valid.drop(columns=['is_root', 'kfold'])\n",
    "    y_train = df_train['is_root']\n",
    "    y_valid = df_valid['is_root']\n",
    "\n",
    "    X_train_processed = X_train_original\n",
    "    X_valid_processed = X_valid_original\n",
    "    \n",
    "    current_feature_columns = list(feature_columns)\n",
    "\n",
    "    if feature_pipeline:\n",
    "        X_train_processed = feature_pipeline.fit_transform(X_train_original)\n",
    "        X_valid_processed = feature_pipeline.transform(X_valid_original)\n",
    "        \n",
    "        if not isinstance(X_train_processed, pd.DataFrame) and hasattr(feature_pipeline, 'get_feature_names_out'):\n",
    "            try:\n",
    "                current_feature_columns = feature_pipeline.get_feature_names_out()\n",
    "                X_train_processed = pd.DataFrame(X_train_processed, columns=current_feature_columns)\n",
    "                X_valid_processed = pd.DataFrame(X_valid_processed, columns=current_feature_columns)\n",
    "            except Exception as e:\n",
    "                if trial: trial.set_user_attr(f'fold_{fold}_feature_pipeline_error', str(e))\n",
    "                print(f\"Fold {fold} error getting feature names from pipeline: {e}\")\n",
    "                return float('nan')\n",
    "        elif not isinstance(X_train_processed, pd.DataFrame):\n",
    "            X_train_processed = pd.DataFrame(X_train_processed, columns=feature_columns)\n",
    "            X_valid_processed = pd.DataFrame(X_valid_processed, columns=feature_columns)\n",
    "\n",
    "    model = clone(estimator)\n",
    "    model.set_params(**params) # random_state is already in params from objective\n",
    "    \n",
    "    try:\n",
    "        model.fit(X_train_processed[current_feature_columns], y_train)\n",
    "    except Exception as e:\n",
    "        if trial: trial.set_user_attr(f'fold_{fold}_fit_error', str(e))\n",
    "        print(f\"Fold {fold} model fitting failed: {str(e)}. Features used: {current_feature_columns}\")\n",
    "        return float('nan')\n",
    "\n",
    "    try:\n",
    "        score = one_root_f1_scorer(\n",
    "            model,\n",
    "            X_valid_processed,\n",
    "            y_valid,\n",
    "            feature_columns_for_model=current_feature_columns\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if trial: trial.set_user_attr(f'fold_{fold}_score_error', str(e))\n",
    "        print(f\"Fold {fold} scoring failed: {str(e)}\")\n",
    "        return float('nan')\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model_performance(model, X_with_groups_and_features, y_true, feature_columns_for_model):\n",
    "    if not isinstance(X_with_groups_and_features, pd.DataFrame):\n",
    "        raise ValueError(\"X_with_groups_and_features must be a pandas DataFrame for this scorer version.\")\n",
    "\n",
    "    # Make predictions using only the specified feature columns\n",
    "    X_for_predict = X_with_groups_and_features[feature_columns_for_model]\n",
    "    try:\n",
    "        proba_values = model.predict_proba(X_for_predict)[:, 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during predict_proba in evaluation: {e}\")\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, {}\n",
    "\n",
    "    # Add probabilities to a copy of the DataFrame\n",
    "    df_copy = X_with_groups_and_features.copy()\n",
    "    df_copy['__proba__'] = proba_values\n",
    "\n",
    "    # Initialize predictions\n",
    "    y_pred = pd.Series(np.zeros(len(df_copy)), index=df_copy.index)\n",
    "    group_cols = ['language', 'sentence']\n",
    "\n",
    "    # Ensure one root per sentence\n",
    "    for _, group_df in df_copy.groupby(group_cols):\n",
    "        if not group_df.empty:\n",
    "            if '__proba__' in group_df.columns:\n",
    "                idx_max = group_df['__proba__'].idxmax()\n",
    "                y_pred.loc[idx_max] = 1\n",
    "            else:\n",
    "                print(f\"Warning: '__proba__' column missing in group_df during evaluation.\")\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_metrics = {\n",
    "        'precision': precision_score(y_true.values, y_pred.values),\n",
    "        'recall': recall_score(y_true.values, y_pred.values),\n",
    "        'f1': f1_score(y_true.values, y_pred.values)\n",
    "    }\n",
    "\n",
    "    # Calculate per-language metrics\n",
    "    per_language_metrics = {}\n",
    "    for lang in X_with_groups_and_features['language'].unique():\n",
    "        lang_mask = X_with_groups_and_features['language'] == lang\n",
    "        lang_true = y_true[lang_mask]\n",
    "        lang_pred = y_pred[lang_mask]\n",
    "        \n",
    "        # Only calculate metrics if there are samples for this language\n",
    "        if len(lang_true) > 0:\n",
    "            per_language_metrics[lang] = {\n",
    "                'precision': precision_score(lang_true, lang_pred),\n",
    "                'recall': recall_score(lang_true, lang_pred),\n",
    "                'f1': f1_score(lang_true, lang_pred)\n",
    "            }\n",
    "\n",
    "    return overall_metrics, per_language_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, df, feature_columns, feature_pipeline=None):\n",
    "    try:\n",
    "        # First decide which configuration to use\n",
    "        config_type = trial.suggest_categorical('config_type', ['l1_l2', 'elasticnet'])\n",
    "        # config_type = trial.suggest_categorical('config_type', ['l1_l2'])\n",
    "        \n",
    "        if config_type == 'l1_l2':\n",
    "            params = {\n",
    "                'penalty': trial.suggest_categorical('penalty', ['l2']),\n",
    "                'C': trial.suggest_float('C', 0.0000001, 100, log=True),\n",
    "                'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "                'class_weight': trial.suggest_categorical('class_weight', ['balanced', {0:1, 1:10}]),\n",
    "                'max_iter': 2000,\n",
    "                'l1_ratio': None\n",
    "            } \n",
    "        else:  # elasticnet\n",
    "            params = {\n",
    "                'penalty': 'elasticnet',\n",
    "                'C': trial.suggest_float('C', 0.0000001, 0.01, log=True),\n",
    "                'solver': 'saga',\n",
    "                'l1_ratio': trial.suggest_float('l1_ratio', 0.1, 0.9),\n",
    "                'class_weight': 'balanced',\n",
    "                'max_iter': 1000\n",
    "            }\n",
    "        \n",
    "        n_folds = 4\n",
    "        df_with_folds = create_folds(df, n_folds, group_column='sentence')\n",
    "        scores = []\n",
    "\n",
    "        for fold in range(n_folds):\n",
    "            trial.set_user_attr(f'fold_{fold+1}_status', 'starting')\n",
    "            try:\n",
    "                score = run_model(\n",
    "                    df_with_folds, \n",
    "                    fold, \n",
    "                    LogisticRegression(), \n",
    "                    params, \n",
    "                    feature_columns, \n",
    "                    feature_pipeline, \n",
    "                    trial=trial\n",
    "                )\n",
    "                \n",
    "                if np.isnan(score):\n",
    "                    trial.set_user_attr(f'fold_{fold+1}_status', 'returned_nan')\n",
    "                    print(f\"Trial {trial.number} Fold {fold+1} returned NaN.\")\n",
    "                else:\n",
    "                    scores.append(score)\n",
    "                    trial.set_user_attr(f'fold_{fold+1}_score', score)\n",
    "                    trial.set_user_attr(f'fold_{fold+1}_status', 'completed')\n",
    "\n",
    "                # Report intermediate value for pruning\n",
    "                trial.report(np.mean(scores), fold)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Trial {trial.number} Fold {fold+1} failed with exception: {str(e)}\")\n",
    "                trial.set_user_attr(f'fold_{fold+1}_status', f'failed_exception: {str(e)}')\n",
    "                continue\n",
    "        \n",
    "        if not scores:\n",
    "            trial.set_user_attr('pruned_reason', 'No valid scores obtained')\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "        mean_score = np.mean([s for s in scores if not np.isnan(s)])\n",
    "        if np.isnan(mean_score):\n",
    "            print(f\"Trial {trial.number} resulted in all NaN scores, returning -inf.\")\n",
    "            return float('-inf')\n",
    "            \n",
    "        return mean_score\n",
    "    except Exception as e:\n",
    "        # print(f\"Trial {trial.number} failed completely with error: {str(e)}\")\n",
    "        trial.set_user_attr('complete_failure', str(e))\n",
    "        return float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv('../../data/train_processed_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['closeness', 'degree', 'pagerank', 'voterank','is_articulation', 'betweenness', 'eigencentrality', 'eccentricity', 'is_japanese',\n",
    "        'lang_group_head_final_sov', 'lang_group_romance_svo', 'lang_group_germanic_v2', 'lang_group_free_order_case', 'lang_group_analytic']\n",
    "\n",
    "# Create study \n",
    "study = optuna.create_study(\n",
    "    direction='maximize'\n",
    ")\n",
    "\n",
    "# Create objective function with fixed parameters\n",
    "objective_func = partial(\n",
    "    objective, \n",
    "    df=train_data, \n",
    "    feature_columns=feature_columns,\n",
    "    feature_pipeline=None\n",
    ")\n",
    "\n",
    "# Optimize with more trials\n",
    "study.optimize(objective_func, n_trials=50, show_progress_bar=True) \n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value (F1 Score): {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance (Validation Set):\n",
      "\n",
      "Overall Performance:\n",
      "Precision: 0.5810\n",
      "Recall: 0.5810\n",
      "F1 Score: 0.5810\n",
      "\n",
      "Per-Language Performance:\n",
      "\n",
      "Japanese:\n",
      "  Precision: 0.0800\n",
      "  Recall: 0.0800\n",
      "  F1 Score: 0.0800\n",
      "\n",
      "Finnish:\n",
      "  Precision: 0.5440\n",
      "  Recall: 0.5440\n",
      "  F1 Score: 0.5440\n",
      "\n",
      "Galician:\n",
      "  Precision: 0.6080\n",
      "  Recall: 0.6080\n",
      "  F1 Score: 0.6080\n",
      "\n",
      "English:\n",
      "  Precision: 0.6240\n",
      "  Recall: 0.6240\n",
      "  F1 Score: 0.6240\n",
      "\n",
      "Hindi:\n",
      "  Precision: 0.1440\n",
      "  Recall: 0.1440\n",
      "  F1 Score: 0.1440\n",
      "\n",
      "French:\n",
      "  Precision: 0.5520\n",
      "  Recall: 0.5520\n",
      "  F1 Score: 0.5520\n",
      "\n",
      "Italian:\n",
      "  Precision: 0.6160\n",
      "  Recall: 0.6160\n",
      "  F1 Score: 0.6160\n",
      "\n",
      "Indonesian:\n",
      "  Precision: 0.8800\n",
      "  Recall: 0.8800\n",
      "  F1 Score: 0.8800\n",
      "\n",
      "Swedish:\n",
      "  Precision: 0.7120\n",
      "  Recall: 0.7120\n",
      "  F1 Score: 0.7120\n",
      "\n",
      "Spanish:\n",
      "  Precision: 0.5920\n",
      "  Recall: 0.5920\n",
      "  F1 Score: 0.5920\n",
      "\n",
      "Icelandic:\n",
      "  Precision: 0.7760\n",
      "  Recall: 0.7760\n",
      "  F1 Score: 0.7760\n",
      "\n",
      "German:\n",
      "  Precision: 0.6400\n",
      "  Recall: 0.6400\n",
      "  F1 Score: 0.6400\n",
      "\n",
      "Korean:\n",
      "  Precision: 0.2160\n",
      "  Recall: 0.2160\n",
      "  F1 Score: 0.2160\n",
      "\n",
      "Polish:\n",
      "  Precision: 0.7600\n",
      "  Recall: 0.7600\n",
      "  F1 Score: 0.7600\n",
      "\n",
      "Thai:\n",
      "  Precision: 0.9440\n",
      "  Recall: 0.9440\n",
      "  F1 Score: 0.9440\n",
      "\n",
      "Turkish:\n",
      "  Precision: 0.2880\n",
      "  Recall: 0.2880\n",
      "  F1 Score: 0.2880\n",
      "\n",
      "Czech:\n",
      "  Precision: 0.6800\n",
      "  Recall: 0.6800\n",
      "  F1 Score: 0.6800\n",
      "\n",
      "Chinese:\n",
      "  Precision: 0.2480\n",
      "  Recall: 0.2480\n",
      "  F1 Score: 0.2480\n",
      "\n",
      "Portuguese:\n",
      "  Precision: 0.5760\n",
      "  Recall: 0.5760\n",
      "  F1 Score: 0.5760\n",
      "\n",
      "Arabic:\n",
      "  Precision: 0.9280\n",
      "  Recall: 0.9280\n",
      "  F1 Score: 0.9280\n",
      "\n",
      "Russian:\n",
      "  Precision: 0.7920\n",
      "  Recall: 0.7920\n",
      "  F1 Score: 0.7920\n"
     ]
    }
   ],
   "source": [
    "n_folds = 4\n",
    "df_with_folds = create_folds(train_data, n_folds, group_column='sentence')\n",
    "\n",
    "# Use the last fold as validation set\n",
    "val_fold = n_folds - 1\n",
    "df_train = df_with_folds[df_with_folds.kfold != val_fold].reset_index(drop=True)\n",
    "df_valid = df_with_folds[df_with_folds.kfold == val_fold].reset_index(drop=True)\n",
    "\n",
    "X_valid = df_valid.drop(columns=['is_root', 'kfold'])\n",
    "y_valid = df_valid['is_root']\n",
    "\n",
    "# Uncomment this if using Optuna\n",
    "# best_model = LogisticRegression(**trial.params)\n",
    "\n",
    "# Best parameters with C > 0.0000001\n",
    "best_model = LogisticRegression(\n",
    "    C=2.7145102022431556e-07, \n",
    "    l1_ratio=0.8868773763606811,\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "# Best parameters with C > 1\n",
    "# best_model = LogisticRegression(\n",
    "#     C=99.86981387827636, \n",
    "#     penalty='l2',\n",
    "#     solver='liblinear',\n",
    "#     class_weight={0: 1, 1: 10},\n",
    "#     max_iter=1000\n",
    "# )\n",
    "\n",
    "feature_columns=['closeness', 'degree', 'pagerank', 'voterank','is_articulation', 'betweenness', 'eigencentrality', 'eccentricity', 'is_japanese',\n",
    "        'lang_group_head_final_sov', 'lang_group_romance_svo', 'lang_group_germanic_v2', 'lang_group_free_order_case', 'lang_group_analytic']\n",
    "\n",
    "best_model.fit(df_train[feature_columns], df_train['is_root'])\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nModel Performance (Validation Set):\")\n",
    "overall_metrics, per_language_metrics = evaluate_model_performance(\n",
    "    best_model, \n",
    "    X_valid, \n",
    "    y_valid,\n",
    "    feature_columns\n",
    ")\n",
    "\n",
    "print(\"\\nOverall Performance:\")\n",
    "print(f\"Precision: {overall_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {overall_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {overall_metrics['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nPer-Language Performance:\")\n",
    "for lang, metrics in per_language_metrics.items():\n",
    "    print(f\"\\n{lang}:\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Metrics:\n",
      "            precision  recall     f1\n",
      "Overall         0.581   0.581  0.581\n",
      "Japanese        0.080   0.080  0.080\n",
      "Finnish         0.544   0.544  0.544\n",
      "Galician        0.608   0.608  0.608\n",
      "English         0.624   0.624  0.624\n",
      "Hindi           0.144   0.144  0.144\n",
      "French          0.552   0.552  0.552\n",
      "Italian         0.616   0.616  0.616\n",
      "Indonesian      0.880   0.880  0.880\n",
      "Swedish         0.712   0.712  0.712\n",
      "Spanish         0.592   0.592  0.592\n",
      "Icelandic       0.776   0.776  0.776\n",
      "German          0.640   0.640  0.640\n",
      "Korean          0.216   0.216  0.216\n",
      "Polish          0.760   0.760  0.760\n",
      "Thai            0.944   0.944  0.944\n",
      "Turkish         0.288   0.288  0.288\n",
      "Czech           0.680   0.680  0.680\n",
      "Chinese         0.248   0.248  0.248\n",
      "Portuguese      0.576   0.576  0.576\n",
      "Arabic          0.928   0.928  0.928\n",
      "Russian         0.792   0.792  0.792\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for overall metrics\n",
    "overall_df = pd.DataFrame([overall_metrics], index=['Overall'])\n",
    "\n",
    "# Create a DataFrame for per-language metrics\n",
    "per_lang_df = pd.DataFrame(per_language_metrics).T\n",
    "\n",
    "# Combine both DataFrames\n",
    "combined_metrics = pd.concat([overall_df, per_lang_df])\n",
    "\n",
    "# Format the metrics to 4 decimal places\n",
    "formatted_metrics = combined_metrics.round(4)\n",
    "\n",
    "# Display the formatted table\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(formatted_metrics.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "# formatted_metrics.to_csv('results.csv')\n",
    "\n",
    "# print(\"Results have been saved to results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model with best parameters...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-9 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-9 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-9 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-9 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-9 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=99.86981387827636, class_weight={0: 1, 1: 10},\n",
       "                   max_iter=1000, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=99.86981387827636, class_weight={0: 1, 1: 10},\n",
       "                   max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=99.86981387827636, class_weight={0: 1, 1: 10},\n",
       "                   max_iter=1000, solver='liblinear')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final model with best parameters and generate predictions\n",
    "print(\"\\nTraining final model with best parameters...\")\n",
    "\n",
    "# Process training data\n",
    "y_train = train_data['is_root']\n",
    "X_train_processed = train_data.drop(columns=['is_root'])\n",
    "\n",
    "# Initialize and train model\n",
    "# Using pop() (does nothing if key is not present)\n",
    "trial.params.pop('config_type', None)\n",
    "\n",
    "# Uncomment this if using Optuna\n",
    "# final_model = LogisticRegression(**trial.params)\n",
    "\n",
    "# Best parameters with C > 0.0000001\n",
    "final_model = LogisticRegression(\n",
    "    C=2.7145102022431556e-07, \n",
    "    l1_ratio=0.8868773763606811,\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "# Best parameters with C > 1\n",
    "final_model = LogisticRegression(\n",
    "    C=99.86981387827636, \n",
    "    penalty='l2',\n",
    "    solver='liblinear',\n",
    "    class_weight={0: 1, 1: 10},\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_processed[feature_columns], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv('../../data/test_processed_random.csv')\n",
    "\n",
    "# Generate predictions\n",
    "test_proba = final_model.predict_proba(test_data[feature_columns])[:, 1]\n",
    "test_data['root_probability'] = test_proba\n",
    "\n",
    "# Create predictions ensuring one root per sentence\n",
    "predictions = pd.Series(np.zeros(len(test_data)), index=test_data.index)\n",
    "group_cols = ['language', 'sentence']\n",
    "\n",
    "for _, group_df in test_data.groupby(group_cols):\n",
    "    if not group_df.empty:\n",
    "        idx_max = group_df['root_probability'].idxmax()\n",
    "        predictions.loc[idx_max] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_file(predictions_df, original_test_data, output_path='submission.csv'):\n",
    "    \"\"\"Generate the final submission file in the required format\"\"\"\n",
    "    # Create a mapping from (language, sentence) to original ID\n",
    "    sentence_lang_to_id = {}\n",
    "    \n",
    "    # Create mapping from (language, sentence) to id\n",
    "    for _, row in original_test_data.drop_duplicates(['language', 'sentence']).iterrows():\n",
    "        sentence_lang_to_id[(row['language'], row['sentence'])] = row['id']\n",
    "    \n",
    "    # Extract sentence IDs and predicted root vertices\n",
    "    submission = []\n",
    "    \n",
    "    for (lang, sent), group in predictions_df.groupby(['language', 'sentence']):\n",
    "        # Get the node with highest probability for this sentence and language\n",
    "        top_node = group.sort_values('root_probability', ascending=False).iloc[0]\n",
    "        \n",
    "        # Use the original id\n",
    "        original_id = sentence_lang_to_id.get((lang, sent))\n",
    "        \n",
    "        submission.append({\n",
    "            'id': original_id,\n",
    "            'root': int(top_node['vertex'])\n",
    "        })\n",
    "    \n",
    "    # Create and sort submission dataframe\n",
    "    submission_df = pd.DataFrame(submission)\n",
    "    submission_df = submission_df.sort_values('id')\n",
    "    \n",
    "    # Save to CSV\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    print(f\"Submission saved to {output_path}\")\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "test_proba = final_model.predict_proba(test_data[feature_columns])[:, 1]\n",
    "test_data['root_probability'] = test_proba\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = []\n",
    "for (lang, sent), group in test_data.groupby(['language', 'sentence']):\n",
    "    # Get the node with highest probability for this sentence and language\n",
    "    top_node = group.loc[group['root_probability'].idxmax()]\n",
    "    submission.append({\n",
    "        'id': top_node['id'],\n",
    "        'root': int(top_node['vertex'])\n",
    "    })\n",
    "\n",
    "# Create and sort submission dataframe\n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df = submission_df.sort_values('id')\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most important features:\n",
      "                       Feature  Coefficient  Abs_Coefficient\n",
      "2                     pagerank    39.331878        39.331878\n",
      "1                       degree   -17.742186        17.742186\n",
      "5                  betweenness     3.326968         3.326968\n",
      "6              eigencentrality     1.429287         1.429287\n",
      "0                    closeness     1.394055         1.394055\n",
      "3                     voterank     0.712704         0.712704\n",
      "4              is_articulation     0.480440         0.480440\n",
      "8                  is_japanese     0.300659         0.300659\n",
      "11      lang_group_germanic_v2    -0.154276         0.154276\n",
      "12  lang_group_free_order_case    -0.121119         0.121119\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using coefficients directly\n",
    "# Get feature names and coefficients\n",
    "feature_names = feature_columns  # or X_train_processed.columns if using pandas\n",
    "coefficients = final_model.coef_[0]  # Get coefficients for first class\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients,\n",
    "    'Abs_Coefficient': np.abs(coefficients)\n",
    "})\n",
    "\n",
    "# Sort by absolute coefficient value\n",
    "feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model intercept: [-3.08824134]\n",
      "\n",
      "Coefficients shape: (1, 14)\n",
      "All coefficients are zero: False\n",
      "\n",
      "Prediction probabilities for first 5 samples:\n",
      "[[0.73267468 0.26732532]\n",
      " [0.93784242 0.06215758]\n",
      " [0.58101784 0.41898216]\n",
      " [0.93417023 0.06582977]\n",
      " [0.7072357  0.2927643 ]\n",
      " [0.27392727 0.72607273]\n",
      " [0.7577484  0.2422516 ]\n",
      " [0.94271431 0.05728569]\n",
      " [0.70641903 0.29358097]\n",
      " [0.63911198 0.36088802]\n",
      " [0.72072594 0.27927406]\n",
      " [0.35271236 0.64728764]\n",
      " [0.35538885 0.64461115]\n",
      " [0.54504052 0.45495948]\n",
      " [0.33849164 0.66150836]\n",
      " [0.67500845 0.32499155]\n",
      " [0.71386899 0.28613101]\n",
      " [0.93234136 0.06765864]\n",
      " [0.53991606 0.46008394]\n",
      " [0.75159328 0.24840672]\n",
      " [0.94195132 0.05804868]\n",
      " [0.82205325 0.17794675]\n",
      " [0.93558184 0.06441816]\n",
      " [0.70060199 0.29939801]\n",
      " [0.9303193  0.0696807 ]\n",
      " [0.63822662 0.36177338]\n",
      " [0.72391175 0.27608825]\n",
      " [0.34735074 0.65264926]\n",
      " [0.6279275  0.3720725 ]\n",
      " [0.91909163 0.08090837]\n",
      " [0.25819169 0.74180831]\n",
      " [0.6279275  0.3720725 ]\n",
      " [0.91909163 0.08090837]\n",
      " [0.90778737 0.09221263]\n",
      " [0.21641074 0.78358926]\n",
      " [0.31158505 0.68841495]\n",
      " [0.63777176 0.36222824]\n",
      " [0.91857873 0.08142127]\n",
      " [0.5193687  0.4806313 ]\n",
      " [0.68487963 0.31512037]\n",
      " [0.92430782 0.07569218]\n",
      " [0.73026918 0.26973082]\n",
      " [0.94151038 0.05848962]\n",
      " [0.94151038 0.05848962]\n",
      " [0.70558607 0.29441393]\n",
      " [0.79032067 0.20967933]\n",
      " [0.26737338 0.73262662]\n",
      " [0.77551289 0.22448711]\n",
      " [0.94063494 0.05936506]\n",
      " [0.73804422 0.26195578]]\n"
     ]
    }
   ],
   "source": [
    "# Check the intercept\n",
    "print(\"Model intercept:\", final_model.intercept_)\n",
    "\n",
    "# Check the coefficients\n",
    "print(\"\\nCoefficients shape:\", final_model.coef_.shape)\n",
    "print(\"All coefficients are zero:\", np.all(final_model.coef_ == 0))\n",
    "\n",
    "# Let's see what predictions look like\n",
    "predictions = final_model.predict(X_train_processed[feature_columns])\n",
    "probabilities = final_model.predict_proba(X_train_processed[feature_columns])\n",
    "\n",
    "print(\"\\nPrediction probabilities for first 5 samples:\")\n",
    "print(probabilities[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique predictions: [0 1]\n",
      "\n",
      "Class distribution in training data:\n",
      "is_root\n",
      "0    0.94683\n",
      "1    0.05317\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check if all predictions are the same\n",
    "unique_predictions = np.unique(predictions)\n",
    "print(\"\\nUnique predictions:\", unique_predictions)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution in training data:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
